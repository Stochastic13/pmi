"""@namespace IMP.pmi.mmcif
   @brief Support for the mmCIF file format.

   IMP has basic support for writing out files in mmCIF format, for
   deposition in [PDB-dev](https://pdb-dev.rcsb.rutgers.edu/).
   mmCIF files are currently generated by creating an
   IMP.pmi.mmcif.ProtocolOutput class, and attaching it to an
   IMP.pmi.representation.Representation object, after which any
   generated models and metadata are collected and output as mmCIF.
"""

from __future__ import print_function
import copy
import IMP.core
import IMP.algebra
import IMP.atom
import IMP.em
import IMP.isd
import IMP.pmi.representation
import IMP.pmi.tools
from IMP.pmi.tools import OrderedDict
import IMP.pmi.output
import re
import ast
import sys
import os
import textwrap
import weakref
import operator
import itertools
import ihm.format
import ihm.location
import ihm.dataset
import ihm.dumper
import ihm.metadata
import ihm.startmodel
import ihm.model

def _assign_id(obj, seen_objs, obj_by_id):
    """Assign a unique ID to obj, and track all ids in obj_by_id."""
    if obj not in seen_objs:
        if not hasattr(obj, 'id'):
            obj_by_id.append(obj)
            obj.id = len(obj_by_id)
        seen_objs[obj] = obj.id
    else:
        obj.id = seen_objs[obj]


def _get_by_residue(p):
    """Determine whether the given particle represents a specific residue
       or a more coarse-grained object."""
    return IMP.atom.Residue.get_is_setup(p) or IMP.atom.Atom.get_is_setup(p)


class _ComponentMapper(object):
    """Map a Particle to a component name"""
    def __init__(self, prot):
        self.o = IMP.pmi.output.Output()
        self.prot = prot
        self.name = 'cif-output'
        self.o.dictionary_pdbs[self.name] = self.prot
        self.o._init_dictchain(self.name, self.prot, multichar_chain=True)

    def __getitem__(self, p):
        protname, is_a_bead = self.o.get_prot_name_from_particle(self.name, p)
        return protname


class _AsymIDMapper(object):
    """Map a Particle to an asym_id (chain ID)"""
    def __init__(self, simo, prot):
        self.simo = simo
        self._cm = _ComponentMapper(prot)

    def __getitem__(self, p):
        protname = self._cm[p]
        return self.simo._get_chain_for_component(protname, self._cm.o)

class _Dumper(object):
    """Base class for helpers to dump output to mmCIF"""
    def __init__(self, simo):
        self.simo = weakref.proxy(simo)

    def finalize(self):
        pass

    def finalize_metadata(self):
        pass


class _AllSoftware(object):
    def __init__(self, system):
        self.system = system
        self.modeller_used = self.phyre2_used = False
        self.system.software.extend([
           ihm.Software(
                 name="Integrative Modeling Platform (IMP)",
                 version=IMP.__version__,
                 classification="integrative model building",
                 description="integrative model building",
                 location='https://integrativemodeling.org'),
           ihm.Software(
                name="IMP PMI module",
                version=IMP.pmi.__version__,
                classification="integrative model building",
                description="integrative model building",
                location='https://integrativemodeling.org')])

    def set_modeller_used(self, version, date):
        if self.modeller_used:
            return
        self.modeller_used = True
        self.system.software.append(ihm.Software(
                    name='MODELLER', classification='comparative modeling',
                    description='Comparative modeling by satisfaction '
                                'of spatial restraints, build ' + date,
                    location='https://salilab.org/modeller/',
                    version=version))

    def set_phyre2_used(self):
        if self.phyre2_used:
            return
        self.phyre2_used = True
        self.system.software.append(ihm.Software(
                   name='Phyre2', classification='protein homology modeling',
                   description='Protein Homology/analogY Recognition '
                               'Engine V 2.0',
                   version='2.0',
                   location='http://www.sbg.bio.ic.ac.uk/~phyre2/'))


class _PDBFragment(object):
    """Record details about part of a PDB file used as input
       for a component."""
    primitive = 'sphere'
    granularity = 'by-residue'
    num = None
    def __init__(self, state, component, start, end, offset, pdbname,
                 chain, hier):
        self.component, self.start, self.end, self.offset, self.pdbname \
              = component, start, end, offset, pdbname
        self.state, self.chain, self.hier = state, chain, hier
        sel = IMP.atom.NonWaterNonHydrogenPDBSelector() \
              & IMP.atom.ChainPDBSelector(chain)
        self.starting_hier = IMP.atom.read_pdb(pdbname, state.m, sel)

    def combine(self, other):
        pass

class _BeadsFragment(object):
    """Record details about beads used to represent part of a component."""
    primitive = 'sphere'
    granularity = 'by-feature'
    chain = None
    def __init__(self, state, component, start, end, num, hier):
        self.state, self.component, self.start, self.end, self.num, self.hier \
              = state, component, start, end, num, hier

    def combine(self, other):
        # todo: don't combine if one fragment is rigid and the other flexible
        if type(other) == type(self) and other.start == self.end + 1:
            self.end = other.end
            self.num += other.num
            return True

class _StartingModel(object):
    """Record details about an input model (e.g. comparative modeling
       template) used for a component."""

    source = ihm.unknown
    db_name = ihm.unknown
    db_code = ihm.unknown
    sequence_identity = ihm.unknown

    def __init__(self, fragment):
        self.fragments = [fragment]
        self.asym_id = fragment.chain

    def get_seq_id_range_all_templates(self):
        """Get the seq_id range covered by all templates in this starting
           model. Where there are multiple templates, consolidate
           them; template info is given in starting_comparative_models."""
        def get_seq_id_range(template, full):
            # The template may cover more than the current starting model
            rng = template.seq_id_range
            return (max(rng[0], full[0]), min(rng[1], full[1]))

        if self.templates:
            full = self.seq_id_begin, self.seq_id_end
            rng = get_seq_id_range(self.templates[0], full)
            for template in self.templates[1:]:
                this_rng = get_seq_id_range(template, full)
                rng = (min(rng[0], this_rng[0]), max(rng[1], this_rng[1]))
            return rng
        else:
            return self.seq_id_begin, self.seq_id_end


class _ModelRepresentationDumper(_Dumper):
    def __init__(self, simo):
        super(_ModelRepresentationDumper, self).__init__(simo)
        # dict of fragments, ordered by representation, then component name,
        # then state
        self.fragments = OrderedDict()
        self.output = IMP.pmi.output.Output()

    def add_fragment(self, state, representation, fragment):
        """Add a model fragment."""
        comp = fragment.component
        if representation not in self.fragments:
            self.fragments[representation] = OrderedDict()
        if comp not in self.fragments[representation]:
            self.fragments[representation][comp] = OrderedDict()
        if state not in self.fragments[representation][comp]:
            self.fragments[representation][comp][state] = []
        fragments = self.fragments[representation][comp][state]
        if len(fragments) == 0 or not fragments[-1].combine(fragment):
            fragments.append(fragment)

    def get_model_mode(self, fragment):
        """Determine the model_mode for a given fragment ('rigid' or
           'flexible')"""
        leaves = IMP.atom.get_leaves(fragment.hier)
        # Assume all leaves are set up as rigid/flexible in the same way
        if IMP.core.RigidMember.get_is_setup(leaves[0]):
            return 'rigid'
        else:
            return 'flexible'

    def _all_fragments(self):
        """Yield all fragments, with copies for any transformed components"""
        trans_orig = {}
        for tc in self.simo._transformed_components:
            if tc.original not in trans_orig:
                trans_orig[tc.original] = []
            trans_orig[tc.original].append(tc.name)
        for representation, compdict in self.fragments.items():
            for comp, statefrag in compdict.items():
                yield representation, comp, comp, statefrag
        for representation, compdict in self.fragments.items():
            for orig, statefrag in compdict.items():
                for comp in trans_orig.get(orig, []):
                    yield representation, comp, orig, statefrag

    def dump(self, writer):
        ordinal_id = 1
        segment_id = 1
        with writer.loop("_ihm_model_representation",
                         ["ordinal_id", "representation_id",
                          "segment_id", "entity_id", "entity_description",
                          "entity_asym_id",
                          "seq_id_begin", "seq_id_end",
                          "model_object_primitive", "starting_model_id",
                          "model_mode", "model_granularity",
                          "model_object_count"]) as l:
            for representation, comp, orig, statefrag in self._all_fragments():
                # For now, assume that representation of the same-named
                # component is the same in all states, so just take the first
                state = list(statefrag.keys())[0]
                chain_id = self.simo._get_chain_for_component(comp, self.output)
                for f in statefrag[state]:
                    entity = self.simo.entities[f.component]
                    starting_model_id = None
                    if hasattr(f, 'pdbname'):
                        starting_model_id \
                             = self.starting_model[state, orig, f.pdbname].name
                    l.write(ordinal_id=ordinal_id,
                            representation_id=representation.id,
                            segment_id=segment_id,
                            entity_id=entity._id,
                            entity_description=entity.description,
                            entity_asym_id=chain_id,
                            seq_id_begin=f.start,
                            seq_id_end=f.end,
                            model_object_primitive=f.primitive,
                            starting_model_id=starting_model_id,
                            model_mode=self.get_model_mode(f),
                            model_granularity=f.granularity,
                            model_object_count=f.num)
                    ordinal_id += 1
                    segment_id += 1

class _DatasetGroup(object):
    """A group of datasets"""
    def __init__(self, datasets):
        self._datasets = list(datasets)

    def finalize(self):
        """Get final datasets for each restraint and remove duplicates"""
        final_datasets = OrderedDict()
        for ds in self._datasets:
            if isinstance(ds, _RestraintDataset):
                d = ds.dataset
            else:
                d = ds
            final_datasets[d] = None
        self._datasets = final_datasets.keys()


class _AllDatasets(object):
    """Track all datasets generated by PMI and add them to the ihm.System"""
    def __init__(self):
        self._datasets = []
        self._datasets_by_state = {}
        self._restraints_by_state = {}
        self._dataset_groups = []

    def get_all_group(self, state):
        """Get a _DatasetGroup encompassing all datasets so far in this state"""
        g = _DatasetGroup(self._datasets_by_state.get(state, [])
                + [r.dataset for r in self._restraints_by_state.get(state, [])])
        self._dataset_groups.append(g)
        return g

    def add(self, state, dataset):
        """Add a new dataset."""
        self._datasets.append(dataset)
        if state not in self._datasets_by_state:
            self._datasets_by_state[state] = []
        self._datasets_by_state[state].append(dataset)
        # We don't add to ihm.System yet, since it can't handle RestraintDataset
        return dataset

    def add_restraint(self, state, restraint):
        """Add the dataset for a restraint"""
        if state not in self._restraints_by_state:
            self._restraints_by_state[state] = []
        self._restraints_by_state[state].append(restraint)

    def _get_final_datasets(self):
        """Get final set of datasets, with RestraintDatasets removed"""
        for d in self._flatten_dataset(self._datasets):
            yield d

    def _get_final_groups(self):
        """Get final set of dataset groups, with RestraintDatasets removed"""
        self._final_dataset_group = {}
        for g in self._dataset_groups:
            g.finalize()
            dg = ihm.dataset.DatasetGroup(g._datasets)
            # Map our temporary class to the ihm object
            self._final_dataset_group[g] = dg
            yield dg

    def _flatten_dataset(self, d):
        if isinstance(d, list):
            for p in d:
                for x in self._flatten_dataset(p):
                    yield x
        elif isinstance(d, _RestraintDataset):
            for x in self._flatten_dataset(d.dataset):
                yield x
        else:
            for p in d.parents:
                for x in self._flatten_dataset(p):
                    yield x
            yield d


class _CrossLinkGroup(object):
    """Group common information for a set of cross links"""
    def __init__(self, pmi_restraint, rdataset):
        self.pmi_restraint, self.rdataset = pmi_restraint, rdataset
        self.label = self.pmi_restraint.label
        # Map commonly-used subtypes to more standard labels
        if self.label == 'wtDSS':
            self.label = 'DSS'


class _ExperimentalCrossLink(object):
    def __init__(self, r1, c1, r2, c2, length, group):
        self.r1, self.c1, self.r2, self.c2 = r1, c1, r2, c2
        self.length, self.group = length, group

class _CrossLink(object):
    def __init__(self, state, ex_xl, p1, p2, sigma1, sigma2, psi):
        self.state = state
        self.ex_xl, self.sigma1, self.sigma2 = ex_xl, sigma1, sigma2
        self.p1, self.p2 = p1, p2
        self.psi = psi

def get_asym_mapper_for_state(simo, state, asym_map):
    asym = asym_map.get(state, None)
    if asym is None:
        asym = _AsymIDMapper(simo, state.prot)
        asym_map[state] = asym
    return asym

class _CrossLinkDumper(_Dumper):
    def __init__(self, simo):
        super(_CrossLinkDumper, self).__init__(simo)
        self.cross_links = []
        self.exp_cross_links = []

    def add_experimental(self, cross_link):
        self.exp_cross_links.append(cross_link)

    def add(self, cross_link):
        self.cross_links.append(cross_link)

    def dump(self, writer):
        self.dump_list(writer)
        self.dump_restraint(writer)
        self.dump_results(writer)

    def dump_list(self, writer):
        seen_cross_links = {}
        with writer.loop("_ihm_cross_link_list",
                         ["id", "group_id", "entity_description_1",
                          "entity_id_1", "seq_id_1", "comp_id_1",
                          "entity_description_2",
                          "entity_id_2", "seq_id_2", "comp_id_2",
                          "linker_type", "dataset_list_id"]) as l:
            xl_id = 0
            for xl in self.exp_cross_links:
                # Skip identical cross links
                sig = (xl.c1, xl.c2, xl.r1, xl.r2, xl.group.label)
                if sig in seen_cross_links:
                    xl.id = seen_cross_links[sig]
                    continue
                entity1 = self.simo.entities[xl.c1]
                entity2 = self.simo.entities[xl.c2]
                seq1 = entity1.sequence
                seq2 = entity2.sequence
                # todo: handle experimental ambiguity (group_id) properly
                xl_id += 1
                seen_cross_links[sig] = xl_id
                xl.id = xl_id
                l.write(id=xl.id, group_id=xl.id,
                        entity_description_1=entity1.description,
                        entity_id_1=entity1._id,
                        seq_id_1=xl.r1,
                        comp_id_1=seq1[xl.r1-1].id,
                        entity_description_2=entity2.description,
                        entity_id_2=entity2._id,
                        seq_id_2=xl.r2,
                        comp_id_2=seq2[xl.r2-1].id,
                        linker_type=xl.group.label,
                        dataset_list_id=xl.group.rdataset.dataset._id)

    def _granularity(self, xl):
        """Determine the granularity of a cross link"""
        if _get_by_residue(xl.p1) and _get_by_residue(xl.p2):
            return 'by-residue'
        else:
            return 'by-feature'

    def dump_restraint(self, writer):
        seen_cross_links = {}
        asym_states = {} # AsymIDMapper for each state
        with writer.loop("_ihm_cross_link_restraint",
                         ["id", "group_id", "entity_id_1", "asym_id_1",
                          "seq_id_1", "comp_id_1",
                          "entity_id_2", "asym_id_2", "seq_id_2", "comp_id_2",
                          "restraint_type", "conditional_crosslink_flag",
                          "model_granularity", "distance_threshold",
                          "psi", "sigma_1", "sigma_2"]) as l:
            xl_id = 0
            for xl in self.cross_links:
                asym = get_asym_mapper_for_state(self.simo, xl.state,
                                                 asym_states)
                entity1 = self.simo.entities[xl.ex_xl.c1]
                entity2 = self.simo.entities[xl.ex_xl.c2]
                seq1 = entity1.sequence
                seq2 = entity2.sequence
                asym1 = asym[xl.p1]
                asym2 = asym[xl.p2]
                # Skip identical cross links
                sig = (asym1, xl.ex_xl.r1, asym2, xl.ex_xl.r2,
                       xl.ex_xl.group.label)
                if sig in seen_cross_links:
                    xl.id = seen_cross_links[sig]
                    continue
                xl_id += 1
                seen_cross_links[sig] = xl_id
                xl.id = xl_id
                l.write(id=xl.id,
                        group_id=xl.ex_xl.id,
                        entity_id_1=entity1._id,
                        asym_id_1=asym1,
                        seq_id_1=xl.ex_xl.r1,
                        comp_id_1=seq1[xl.ex_xl.r1-1].id,
                        entity_id_2=entity2._id,
                        asym_id_2=asym2,
                        seq_id_2=xl.ex_xl.r2,
                        comp_id_2=seq2[xl.ex_xl.r2-1].id,
                        restraint_type='upper bound',
                        # todo: any circumstances where this could be ANY?
                        conditional_crosslink_flag="ALL",
                        model_granularity=self._granularity(xl),
                        distance_threshold=xl.ex_xl.length,
                        psi=xl.psi.get_scale(),
                        sigma_1=xl.sigma1.get_scale(),
                        sigma_2=xl.sigma2.get_scale())

    def _set_psi_sigma(self, model, g):
        for resolution in g.pmi_restraint.sigma_dictionary:
            statname = 'ISDCrossLinkMS_Sigma_%s_%s' % (resolution, g.label)
            if model.stats and statname in model.stats:
                sigma = float(model.stats[statname])
                p = g.pmi_restraint.sigma_dictionary[resolution][0]
                p.set_scale(sigma)
        for psiindex in g.pmi_restraint.psi_dictionary:
            statname = 'ISDCrossLinkMS_Psi_%s_%s' % (psiindex, g.label)
            if model.stats and statname in model.stats:
                psi = float(model.stats[statname])
                p = g.pmi_restraint.psi_dictionary[psiindex][0]
                p.set_scale(psi)

    def dump_results(self, writer):
        all_groups = {}
        for xl in self.cross_links:
            all_groups[xl.ex_xl.group] = None
        ordinal = 1
        with writer.loop("_ihm_cross_link_result_parameters",
                         ["ordinal_id", "restraint_id", "model_id",
                          "psi", "sigma_1", "sigma_2"]) as l:
            for model in self.models:
                if not model._is_restrained: continue
                for g in all_groups.keys():
                    self._set_psi_sigma(model, g)
                for xl in self.cross_links:
                    if model.stats:
                        l.write(ordinal_id=ordinal, restraint_id=xl.id,
                                model_id=model.id, psi=xl.psi.get_scale(),
                                sigma_1=xl.sigma1.get_scale(),
                                sigma_2=xl.sigma2.get_scale())
                        ordinal += 1

class _EM2DRestraint(ihm.restraint.EM2DRestraint):
    def __init__(self, state, pmi_restraint, image_number, resolution,
                 pixel_size, image_resolution, projection_number,
                 micrographs_number):
        self.pmi_restraint, self.image_number = pmi_restraint, image_number
        super(_EM2DRestraint, self).__init__(
                dataset=pmi_restraint.datasets[image_number],
                assembly=state.modeled_assembly,
                segment=False, number_raw_micrographs=micrographs_number,
                pixel_size_width=pixel_size, pixel_size_height=pixel_size,
                image_resolution=image_resolution,
                number_of_projections=projection_number)

    # Have our dataset point to that in the original PMI restraint
    def __get_dataset(self):
        return self.pmi_restraint.datasets[self.image_number]
    def __set_dataset(self, val):
        self.pmi_restraint.datasets[self.image_number] = val
    dataset = property(__get_dataset, __set_dataset)

    def add_fits_from_model_statfile(self, model):
        ccc = self._get_cross_correlation(model)
        transform = self._get_transformation(model)
        rot = transform.get_rotation()
        rm = [[e for e in rot.get_rotation_matrix_row(i)] for i in range(3)]
        self.fits[model] = ihm.restraint.EM2DRestraintFit(
                               cross_correlation_coefficient=ccc,
                               rot_matrix=rm,
                               tr_vector=transform.get_translation())

    def _get_transformation(self, model):
        """Get the transformation that places the model on the image"""
        stats = model.em2d_stats or model.stats
        prefix = 'ElectronMicroscopy2D_%s_Image%d' % (self.pmi_restraint.label,
                                                      self.image_number + 1)
        r = [float(stats[prefix + '_Rotation%d' % i]) for i in range(4)]
        t = [float(stats[prefix + '_Translation%d' % i])
             for i in range(3)]
        # If the model coordinates are transformed, need to back transform
        # them first
        inv = model.transform.get_inverse()
        return IMP.algebra.Transformation3D(IMP.algebra.Rotation3D(*r),
                                            IMP.algebra.Vector3D(*t)) * inv

    def _get_cross_correlation(self, model):
        """Get the cross correlation coefficient between the model projection
           and the image"""
        stats = model.em2d_stats or model.stats
        return float(stats['ElectronMicroscopy2D_%s_Image%d_CCC'
                                 % (self.pmi_restraint.label,
                                    self.image_number + 1)])


class _EM3DRestraint(ihm.restraint.EM3DRestraint):

    def __init__(self, simo, state, pmi_restraint, target_ps, densities):
        self.pmi_restraint = pmi_restraint
        super(_EM3DRestraint, self).__init__(
                dataset=pmi_restraint.dataset,
                assembly=self._get_assembly(densities, simo, state),
                fitting_method='Gaussian mixture models',
                number_of_gaussians=len(target_ps))

    # Have our dataset point to that in the original PMI restraint
    def __set_dataset(self, val):
        self.pmi_restraint.dataset = val
    dataset = property(lambda self: self.pmi_restraint.dataset,
                       __set_dataset)

    def _get_assembly(self, densities, simo, state):
        """Get the Assembly that this restraint acts on"""
        cm = _ComponentMapper(state.prot)
        components = {}
        for d in densities:
            components[cm[d]] = None # None == all residues in this component
        a = simo._get_subassembly(components,
                              name="EM subassembly",
                              description="All components that fit the EM map")
        return a

    def add_fits_from_model_statfile(self, model):
        ccc = self._get_cross_correlation(model)
        self.fits[model] = ihm.restraint.EM3DRestraintFit(
                                           cross_correlation_coefficient=ccc)

    def _get_cross_correlation(self, model):
        """Get the cross correlation coefficient between the model
           and the map"""
        if model.stats is not None:
            return float(model.stats['GaussianEMRestraint_%s_CCC'
                                     % self.pmi_restraint.label])


class _Protocol(list):
    """A modeling protocol. This can consist of multiple _ProtocolSteps."""
    pass

class _ProtocolStep(object):
    """A single step in a _Protocol."""
    pass

class _ReplicaExchangeProtocolStep(_ProtocolStep):
    def __init__(self, state, rex):
        self.state = state
        self.modeled_assembly = state.modeled_assembly
        self.name = 'Sampling'
        if rex.monte_carlo_sample_objects is not None:
            self.method = 'Replica exchange monte carlo'
        else:
            self.method = 'Replica exchange molecular dynamics'
        self.num_models_end = rex.vars["number_of_frames"]


class _SimpleProtocolStep(_ProtocolStep):
    def __init__(self, state, num_models_end, method):
        self.state = state
        self.modeled_assembly = state.modeled_assembly
        self.name = 'Sampling'
        self.method = method
        self.num_models_end = num_models_end


class _Chain(object):
    """Represent a single chain in a Model"""
    def __init__(self, pmi_chain_id, asym_unit):
        self.pmi_chain_id, self.asym_unit = pmi_chain_id, asym_unit
        self.spheres = []
        self.atoms = []

    def add(self, xyz, atom_type, residue_type, residue_index,
            all_indexes, radius):
        if atom_type is None:
            self.spheres.append((xyz, residue_type, residue_index,
                                 all_indexes, radius))
        else:
            self.atoms.append((xyz, atom_type, residue_type, residue_index,
                               all_indexes, radius))
    orig_comp = property(lambda self: self.comp)

class _TransformedChain(object):
    """Represent a chain that is a transformed version of another"""
    def __init__(self, orig_chain, asym_unit, transform):
        self.orig_chain, self.asym_unit = orig_chain, asym_unit
        self.transform = transform

    def __get_spheres(self):
        for (xyz, residue_type, residue_index, all_indexes,
             radius) in self.orig_chain.spheres:
            yield (self.transform * xyz, residue_type, residue_index,
                   all_indexes, radius)
    spheres = property(__get_spheres)

    def __get_atoms(self):
        for (xyz, atom_type, residue_type, residue_index, all_indexes,
             radius) in self.orig_chain.atoms:
            yield (self.transform * xyz, atom_type, residue_type, residue_index,
                   all_indexes, radius)
    atoms = property(__get_atoms)

    entity = property(lambda self: self.orig_chain.entity)
    orig_comp = property(lambda self: self.orig_chain.comp)

class _Excluder(object):
    def __init__(self, component, simo):
        self._seqranges = simo._exclude_coords.get(component, [])

    def is_excluded(self, indexes):
        """Return True iff the given sequence range is excluded."""
        for seqrange in self._seqranges:
            if indexes[0] >= seqrange[0] and indexes[-1] <= seqrange[1]:
                return True


class _Model(object):
    def __init__(self, prot, simo, protocol, assembly, representation, group):
        # Transformation from IMP coordinates into mmCIF coordinate system.
        # Normally we pass through coordinates unchanged, but in some cases
        # we may want to translate them (e.g. Nup84, where the deposited PDB
        # files are all centered; we want the mmCIF files to match)
        self.transform = IMP.algebra.get_identity_transformation_3d()
        self.group = group
        # The _Protocol which produced this model
        self.protocol = protocol
        self.assembly = assembly
        self.representation = representation
        self.em2d_stats = None
        self.stats = None
        # True iff restraints act on this model
        self._is_restrained = True
        o = self.output = IMP.pmi.output.Output(atomistic=True)
        name = 'cif-output'
        o.dictionary_pdbs[name] = prot
        o._init_dictchain(name, prot, multichar_chain=True)
        (particle_infos_for_pdb,
         self.geometric_center) = o.get_particle_infos_for_pdb_writing(name)
        self.geometric_center = IMP.algebra.Vector3D(*self.geometric_center)
        self._make_spheres_atoms(particle_infos_for_pdb, o, name, simo)
        self.rmsf = {}
        self.name = None

    def all_chains(self, simo):
        """Yield all chains, including transformed ones"""
        chain_for_comp = {}
        for c in self.chains:
            yield c
            chain_for_comp[c.comp] = c
        for tc in simo._transformed_components:
            orig_chain = chain_for_comp.get(tc.original, None)
            if orig_chain:
                asym = simo.asym_units[tc.name]
                c = _TransformedChain(orig_chain, asym, tc.transform)
                c.comp = tc.name
                yield c

    def _make_spheres_atoms(self, particle_infos_for_pdb, o, name, simo):
        entity_for_chain = {}
        comp_for_chain = {}
        correct_asym = {}
        for protname, chain_id in o.dictchain[name].items():
            entity_for_chain[chain_id] = simo.entities[protname]
            comp_for_chain[chain_id] = protname
            # When doing multi-state modeling, the chain ID returned here
            # (assigned sequentially) might not be correct (states may have
            # gaps in the chain IDs). Map it to the correct asym unit.
            correct_asym[chain_id] = simo.asym_units[protname]

        # Gather by chain ID (should be sorted by chain ID already)
        self.chains = []
        chain = None
        excluder = None

        for (xyz, atom_type, residue_type, chain_id, residue_index,
             all_indexes, radius) in particle_infos_for_pdb:
            if chain is None or chain.pmi_chain_id != chain_id:
                chain = _Chain(chain_id, correct_asym[chain_id])
                chain.entity = entity_for_chain[chain_id]
                chain.comp = comp_for_chain[chain_id]
                self.chains.append(chain)
                excluder = _Excluder(chain.comp, simo)
            if not excluder.is_excluded(all_indexes if all_indexes
                                        else [residue_index]):
                chain.add(xyz, atom_type, residue_type, residue_index,
                          all_indexes, radius)

    def parse_rmsf_file(self, fname, component):
        self.rmsf[component] = rmsf = {}
        with open(fname) as fh:
            for line in fh:
                resnum, blocknum, val = line.split()
                rmsf[int(resnum)] = (int(blocknum), float(val))

    def get_rmsf(self, component, indexes):
        """Get the RMSF value for the given residue indexes."""
        if not self.rmsf:
            return None
        rmsf = self.rmsf[component]
        blocknums = dict.fromkeys(rmsf[ind][0] for ind in indexes)
        if len(blocknums) != 1:
            raise ValueError("Residue indexes %s aren't all in the same block"
                             % str(indexes))
        return rmsf[indexes[0]][1]

class _ModelDumper(_Dumper):
    def __init__(self, simo):
        super(_ModelDumper, self).__init__(simo)
        self.models = []

    def add(self, prot, protocol, assembly, representation, group):
        m = _Model(prot, self.simo, protocol, assembly, representation, group)
        self.models.append(m)
        m.id = len(self.models)
        m._id = m.id # support using ihm.restraint classes
        return m

    def dump(self, writer):
        self.dump_model_list(writer)
        self.dump_atoms(writer)
        self.dump_spheres(writer)

    def dump_model_list(self, writer):
        ordinal = 1
        with writer.loop("_ihm_model_list",
                         ["ordinal_id", "model_id", "model_group_id",
                          "model_name", "model_group_name", "assembly_id",
                          "protocol_id", "representation_id"]) as l:
            for model in self.models:
                l.write(ordinal_id=ordinal, model_id=model.id,
                        model_group_id=model.group._id,
                        model_name=model.name,
                        model_group_name=model.group.name,
                        assembly_id=model.assembly._id,
                        protocol_id=model.protocol.id,
                        representation_id=model.representation.id)
                ordinal += 1

    def dump_atoms(self, writer):
        ordinal = 1
        with writer.loop("_atom_site",
                         ["id", "label_atom_id", "label_comp_id",
                          "label_seq_id",
                          "label_asym_id", "Cartn_x",
                          "Cartn_y", "Cartn_z", "label_entity_id",
                          "model_id"]) as l:
            for model in self.models:
                for chain in model.all_chains(self.simo):
                    for atom in chain.atoms:
                        (xyz, atom_type, residue_type, residue_index,
                         all_indexes, radius) = atom
                        pt = model.transform * xyz
                        l.write(id=ordinal,
                                label_atom_id=atom_type.get_string(),
                                label_comp_id=residue_type.get_string(),
                                label_asym_id=chain.asym_unit._id,
                                label_entity_id=chain.entity._id,
                                label_seq_id=residue_index,
                                Cartn_x=pt[0], Cartn_y=pt[1], Cartn_z=pt[2],
                                model_id=model.id)
                        ordinal += 1

    def dump_spheres(self, writer):
        ordinal = 1
        with writer.loop("_ihm_sphere_obj_site",
                         ["ordinal_id", "entity_id", "seq_id_begin",
                          "seq_id_end", "asym_id", "Cartn_x",
                          "Cartn_y", "Cartn_z", "object_radius", "rmsf",
                          "model_id"]) as l:
            for model in self.models:
                for chain in model.all_chains(self.simo):
                    for sphere in chain.spheres:
                        (xyz, residue_type, residue_index,
                         all_indexes, radius) = sphere
                        if all_indexes is None:
                            all_indexes = (residue_index,)
                        pt = model.transform * xyz
                        l.write(ordinal_id=ordinal,
                                entity_id=chain.entity._id,
                                seq_id_begin = all_indexes[0],
                                seq_id_end = all_indexes[-1],
                                asym_id=chain.asym_unit._id,
                                Cartn_x=pt[0], Cartn_y=pt[1], Cartn_z=pt[2],
                                object_radius=radius,
                                rmsf=model.get_rmsf(chain.orig_comp,
                                                    all_indexes),
                                model_id=model.id)
                        ordinal += 1


class _ModelProtocolDumper(_Dumper):
    def __init__(self, simo):
        super(_ModelProtocolDumper, self).__init__(simo)
        # Lists of Protocols by state
        self.protocols = OrderedDict()
        self._protocol_id = 1

    def add_protocol(self, state):
        """Add a new Protocol"""
        if state not in self.protocols:
            self.protocols[state] = []
        p = _Protocol()
        self.protocols[state].append(p)
        p.id = self._protocol_id # IDs must be unique across states
        self._protocol_id += 1

    def add_step(self, step):
        """Add a ProtocolStep to the last Protocol"""
        state = step.state
        if state not in self.protocols:
            self.add_protocol(state)
        protocol = self.get_last_protocol(state)
        if len(protocol) == 0:
            step.num_models_begin = 0
        else:
            step.num_models_begin = protocol[-1].num_models_end
        protocol.append(step)
        step.id = len(protocol)
        # Assume that protocol uses all currently-defined datasets
        step.dataset_group = self.simo.all_datasets.get_all_group(state)

    def get_last_protocol(self, state):
        """Return the most recently-added _Protocol"""
        return self.protocols[state][-1]

    def dump(self, writer):
        ordinal = 1
        with writer.loop("_ihm_modeling_protocol",
                         ["ordinal_id", "protocol_id", "step_id",
                          "struct_assembly_id", "dataset_group_id",
                          "struct_assembly_description", "protocol_name",
                          "step_name", "step_method", "num_models_begin",
                          "num_models_end", "multi_scale_flag",
                          "multi_state_flag", "ordered_flag"]) as l:
            for ps in self.protocols.values():
                for p in ps:
                    for step in p:
                        # Map to final dataset group; todo: remove
                        dg = step.dataset_group
                        dg = self.simo.all_datasets._final_dataset_group[dg]
                        l.write(ordinal_id=ordinal, protocol_id=p.id,
                                step_id=step.id, step_method=step.method,
                                step_name=step.name,
                                struct_assembly_id=step.modeled_assembly._id,
                                dataset_group_id=dg._id,
                                num_models_begin=step.num_models_begin,
                                num_models_end=step.num_models_end,
                                # todo: support multiple states, time ordered
                                multi_state_flag=False, ordered_flag=False,
                                # all PMI models are multi scale
                                multi_scale_flag=True)
                        ordinal += 1


class _MSESeqDif(object):
    """Track an MSE -> MET mutation in the starting model sequence"""
    comp_id = 'MET'
    db_comp_id = 'MSE'
    details = 'Conversion of modified residue MSE to MET'
    def __init__(self, res, component, asym_id, model, offset):
        self.res, self.component, self.asym_id = res, component, asym_id
        self.model = model
        self.offset = offset


class _StartingModelDumper(_Dumper):
    def __init__(self, simo):
        super(_StartingModelDumper, self).__init__(simo)
        # dict of starting models (entire PDB files), collected from fragments,
        # ordered by component name and state
        self.models = OrderedDict()
        # mapping from state+component+pdbname to starting model
        self.starting_model = {}
        self.output = IMP.pmi.output.Output()

    def add_pdb_fragment(self, fragment):
        """Add a starting model PDB fragment."""
        comp = fragment.component
        state = fragment.state
        if comp not in self.models:
            self.models[comp] = OrderedDict()
        if state not in self.models[comp]:
            self.models[comp][state] = []
        models = self.models[comp][state]
        if len(models) == 0 \
           or models[-1].fragments[0].pdbname != fragment.pdbname:
            model = _StartingModel(fragment)
            models.append(model)
            self.get_model_metadata(model, fragment.pdbname, fragment.chain)
        else:
            models[-1].fragments.append(fragment)

    def get_model_metadata(self, model, pdbname, chain):
        parser = ihm.metadata.PDBParser()
        r = parser.parse_file(pdbname)

        model.dataset = r['dataset']
        self.simo.system.software.extend(r['software'])
        self.simo._add_dataset(model.dataset)
        model.templates = r['templates']
        model.metadata = r['metadata']
        for t in model.templates:
            if t.alignment_file:
                self.simo.system.locations.append(t.alignment_file)
            if t.dataset:
                self.simo._add_dataset(t.dataset)

    def assign_model_details(self):
        for comp, states in self.models.items():
            model_id = 0
            for state in states:
                for model in states[state]:
                    model_id += 1
                    model.name = "%s-m%d" % (comp, model_id)
                    self.starting_model[state, comp,
                                        model.fragments[0].pdbname] = model
                    model.seq_id_begin = min(x.start + x.offset
                                             for x in model.fragments)
                    model.seq_id_end = max(x.end + x.offset
                                           for x in model.fragments)

    def all_models(self):
        for comp, states in self.models.items():
            # For now, assume that starting model of the same-named
            # component is the same in all states, so just take the first
            first_state = list(states.keys())[0]
            for model in states[first_state]:
                yield model

    def finalize(self):
        self.assign_model_details()

    def dump(self, writer):
        self.dump_details(writer)
        self.dump_comparative(writer)
        seq_dif = self.dump_coords(writer)
        self.dump_seq_dif(writer, seq_dif)

    def dump_seq_dif(self, writer, seq_dif):
        ordinal = 1
        with writer.loop("_ihm_starting_model_seq_dif",
                     ["ordinal_id", "entity_id", "asym_id",
                      "seq_id", "comp_id", "starting_model_id",
                      "db_asym_id", "db_seq_id", "db_comp_id",
                      "details"]) as l:
            for sd in seq_dif:
                chain_id = self.simo._get_chain_for_component(
                                    sd.component, self.output)
                entity = self.simo.entities[sd.component]
                l.write(ordinal_id=ordinal, entity_id=entity._id,
                        asym_id=chain_id, seq_id=sd.res.get_index(),
                        comp_id=sd.comp_id,
                        db_asym_id=sd.asym_id,
                        db_seq_id=sd.res.get_index() - sd.offset,
                        db_comp_id=sd.db_comp_id,
                        starting_model_id=sd.model.name,
                        details=sd.details)
                ordinal += 1

    def dump_comparative(self, writer):
        """Dump details on comparative models. Must be called after
           dump_details() since it uses IDs assigned there."""
        with writer.loop("_ihm_starting_comparative_models",
                     ["ordinal_id", "starting_model_id",
                      "starting_model_auth_asym_id",
                      "starting_model_seq_id_begin",
                      "starting_model_seq_id_end",
                      "template_auth_asym_id", "template_seq_id_begin",
                      "template_seq_id_end", "template_sequence_identity",
                      "template_sequence_identity_denominator",
                      "template_dataset_list_id",
                      "alignment_file_id"]) as l:
            ordinal = 1
            for model in self.all_models():
                for template in model.templates:
                    denom = template.sequence_identity_denominator
                    l.write(ordinal_id=ordinal,
                      starting_model_id=model.name,
                      starting_model_auth_asym_id=model.asym_id,
                      starting_model_seq_id_begin=template.seq_id_range[0],
                      starting_model_seq_id_end=template.seq_id_range[1],
                      template_auth_asym_id=template.asym_id,
                      template_seq_id_begin=template.template_seq_id_range[0],
                      template_seq_id_end=template.template_seq_id_range[1],
                      template_sequence_identity=template.sequence_identity,
                      template_sequence_identity_denominator=denom,
                      template_dataset_list_id=template.dataset._id
                                               if template.dataset
                                               else ihm.unknown,
                      alignment_file_id=template.alignment_file._id
                                        if template.alignment_file
                                        else ihm.unknown)
                    ordinal += 1

    def dump_details(self, writer):
        # Map dataset types to starting model sources
        source_map = {'Comparative model': 'comparative model',
                      'Integrative model': 'integrative model',
                      'Experimental model': 'experimental model'}
        writer.write_comment("""IMP will attempt to identify which input models
are crystal structures and which are comparative models, but does not always
have sufficient information to deduce all of the templates used for comparative
modeling. These may need to be added manually below.""")
        with writer.loop("_ihm_starting_model_details",
                     ["starting_model_id", "entity_id", "entity_description",
                      "asym_id", "seq_id_begin",
                      "seq_id_end", "starting_model_source",
                      "starting_model_auth_asym_id",
                      "starting_model_sequence_offset",
                      "dataset_list_id"]) as l:
            for model in self.all_models():
                f = model.fragments[0]
                entity = self.simo.entities[f.component]
                chain_id = self.simo._get_chain_for_component(f.component,
                                                              self.output)
                seq_id_range = model.get_seq_id_range_all_templates()
                l.write(entity_id=entity._id,
                      entity_description=entity.description,
                      asym_id=chain_id,
                      seq_id_begin=seq_id_range[0],
                      seq_id_end=seq_id_range[1],
                      starting_model_auth_asym_id=model.asym_id,
                      starting_model_id=model.name,
                      starting_model_source=source_map[model.dataset.data_type],
                      starting_model_sequence_offset=f.offset,
                      dataset_list_id=model.dataset._id)

    def dump_coords(self, writer):
        seq_dif = []
        ordinal = 1
        with writer.loop("_ihm_starting_model_coord",
                     ["starting_model_id", "group_PDB", "id", "type_symbol",
                      "atom_id", "comp_id", "entity_id", "asym_id",
                      "seq_id", "Cartn_x",
                      "Cartn_y", "Cartn_z", "B_iso_or_equiv",
                      "ordinal_id"]) as l:
            for model in self.all_models():
                for f in model.fragments:
                    sel = IMP.atom.Selection(f.starting_hier,
                               residue_indexes=list(range(f.start - f.offset,
                                                       f.end - f.offset + 1)))
                    last_res_index = None
                    for a in sel.get_selected_particles():
                        coord = IMP.core.XYZ(a).get_coordinates()
                        atom = IMP.atom.Atom(a)
                        element = atom.get_element()
                        element = IMP.atom.get_element_table().get_name(element)
                        atom_name = atom.get_atom_type().get_string()
                        group_pdb = 'ATOM'
                        if atom_name.startswith('HET:'):
                            group_pdb = 'HETATM'
                            atom_name = atom_name[4:]
                        res = IMP.atom.get_residue(atom)
                        res_name = res.get_residue_type().get_string()
                        # MSE in the original PDB is automatically mutated
                        # by IMP to MET, so reflect that in the output,
                        # and pass back to populate the seq_dif category.
                        if res_name == 'MSE':
                            res_name = 'MET'
                            # Only add one seq_dif record per residue
                            ind = res.get_index()
                            if ind != last_res_index:
                                last_res_index = ind
                                # This should only happen when we're using
                                # a crystal structure as the source (a
                                # comparative model would use MET in
                                # the sequence)
                                assert(len(model.templates) == 0)
                                seq_dif.append(_MSESeqDif(res, f.component,
                                                          model.asym_id,
                                                          model, f.offset))
                        chain_id = self.simo._get_chain_for_component(
                                            f.component, self.output)
                        entity = self.simo.entities[f.component]
                        l.write(starting_model_id=model.name,
                                group_PDB=group_pdb,
                                id=atom.get_input_index(), type_symbol=element,
                                atom_id=atom_name, comp_id=res_name,
                                entity_id=entity._id,
                                asym_id=chain_id,
                                seq_id=res.get_index() + f.offset,
                                Cartn_x=coord[0],
                                Cartn_y=coord[1], Cartn_z=coord[2],
                                B_iso_or_equiv=atom.get_temperature_factor(),
                                ordinal_id=ordinal)
                        ordinal += 1
        return seq_dif

class _StructConfDumper(_Dumper):
    def all_rigid_fragments(self):
        """Yield all rigid model representation fragments"""
        asym_states = {}
        model_repr = self.simo.model_repr_dump
        for representation, compdict in model_repr.fragments.items():
            for comp, statefrag in compdict.items():
                # For now, assume that representation of the same-named
                # component is the same in all states, so just take the first
                state = list(statefrag.keys())[0]
                for f in statefrag[state]:
                    if hasattr(f, 'pdbname') \
                       and model_repr.get_model_mode(f) == 'rigid':
                        asym = get_asym_mapper_for_state(self.simo, f.state,
                                                         asym_states)
                        yield (f, model_repr.starting_model[state, comp,
                                                            f.pdbname],
                               asym[f.hier])

    def all_helices(self):
        """Yield all helices that overlap with rigid model fragments"""
        for f, starting_model, asym_id in self.all_rigid_fragments():
            if len(starting_model.templates) == 0:
                for m in starting_model.metadata:
                    if isinstance(m, ihm.startmodel.PDBHelix) \
                       and m.start_asym == starting_model.asym_id \
                       and m.end_asym == starting_model.asym_id \
                       and m.start_resnum >= f.start and m.end_resnum <= f.end:
                        yield (m, max(f.start, m.start_resnum),
                               min(f.end, m.end_resnum), asym_id)

    def dump(self, writer):
        with writer.category("_struct_conf_type") as l:
            l.write(id='HELX_P', criteria=ihm.unknown,
                    reference=ihm.unknown)
        # Dump helix information for the model. For any model fragment that
        # is rigid, atomic, and uses an experimental PDB structure as the
        # starting model, inherit any helix information from that PDB file.
        # Note that we can't use the helix id from the original PDB, since
        # it has to be unique and this might not be the case if we inherit
        # from multiple PDBs.
        ordinal = 0
        with writer.loop("_struct_conf",
                     ["id", "conf_type_id", "beg_label_comp_id",
                      "beg_label_asym_id", "beg_label_seq_id",
                      "end_label_comp_id", "end_label_asym_id",
                      "end_label_seq_id",]) as l:
            for h, begin, end, asym_id in self.all_helices():
                ordinal += 1
                l.write(id='HELX_P%d' % ordinal, conf_type_id='HELX_P',
                        beg_label_comp_id=h.start_resnam,
                        beg_label_asym_id=asym_id,
                        beg_label_seq_id=begin,
                        end_label_comp_id=h.end_resnam,
                        end_label_asym_id=asym_id,
                        end_label_seq_id=end)


class _PostProcess(object):
    """Base class for any post processing"""
    pass

class _ReplicaExchangeAnalysisPostProcess(_PostProcess):
    """Post processing using AnalysisReplicaExchange0 macro"""
    type = 'cluster'
    feature = 'RMSD'

    def __init__(self, protocol, rex, num_models_begin):
        self.protocol = protocol
        self.rex = rex
        self.num_models_begin = num_models_begin
        self.num_models_end = 0
        for fname in self.get_all_stat_files():
            with open(fname) as fh:
                self.num_models_end += len(fh.readlines())

    def get_stat_file(self, cluster_num):
        return os.path.join(self.rex._outputdir, "cluster.%d" % cluster_num,
                            'stat.out')

    def get_all_stat_files(self):
        for i in range(self.rex._number_of_clusters):
            yield self.get_stat_file(i)

class _SimplePostProcess(_PostProcess):
    """Simple ad hoc clustering"""
    type = 'cluster'
    feature = 'RMSD'

    def __init__(self, protocol, num_models_begin, num_models_end):
        self.protocol = protocol
        self.num_models_begin = num_models_begin
        self.num_models_end = num_models_end


class _NoPostProcess(_PostProcess):
    """No post processing"""
    type = 'none'
    feature = 'none'

    def __init__(self, protocol, num_models):
        self.protocol = protocol
        self.num_models_begin = self.num_models_end = num_models

class _PostProcessDumper(_Dumper):
    def __init__(self, simo):
        super(_PostProcessDumper, self).__init__(simo)
        self.postprocs = []

    def add(self, postproc):
        protocol = postproc.protocol
        self.postprocs.append(postproc)
        postproc.id = len(self.postprocs)
        postproc._id = postproc.id # work with python-ihm EnsembleDumper

    def finalize(self):
        # Assign step IDs per protocol
        pp_by_protocol = {}
        for p in self.postprocs:
            protocol = p.protocol
            if id(protocol) not in pp_by_protocol:
                pp_by_protocol[id(protocol)] = []
            by_prot = pp_by_protocol[id(protocol)]
            by_prot.append(p)
            p.step_id = len(by_prot)

    def dump(self, writer):
        with writer.loop("_ihm_modeling_post_process",
                         ["id", "protocol_id", "analysis_id", "step_id",
                          "type", "feature", "num_models_begin",
                          "num_models_end"]) as l:
            # todo: handle multiple analyses
            for p in self.postprocs:
                l.write(id=p.id, protocol_id=p.protocol.id, analysis_id=1,
                        step_id=p.step_id, type=p.type, feature=p.feature,
                        num_models_begin=p.num_models_begin,
                        num_models_end=p.num_models_end)

class _ReplicaExchangeAnalysisEnsemble(ihm.model.Ensemble):
    """Ensemble generated using AnalysisReplicaExchange0 macro"""

    num_models_deposited = None # Override base class property

    def __init__(self, pp, cluster_num, model_group, num_deposit):
        with open(pp.get_stat_file(cluster_num)) as fh:
            num_models = len(fh.readlines())
        super(_ReplicaExchangeAnalysisEnsemble, self).__init__(
                num_models=num_models,
                model_group=model_group, post_process=pp,
                clustering_feature=pp.feature,
                name="cluster %d" % (cluster_num + 1))
        self.cluster_num = cluster_num
        self.num_models_deposited = num_deposit

    def get_rmsf_file(self, component):
        return os.path.join(self.post_process.rex._outputdir,
                            'cluster.%d' % self.cluster_num,
                            'rmsf.%s.dat' % component)

    def load_rmsf(self, model, component):
        fname = self.get_rmsf_file(component)
        if os.path.exists(fname):
            model.parse_rmsf_file(fname, component)

    def get_localization_density_file(self, component):
        # todo: this assumes that the localization density file name matches
        # the component name and is of the complete residue range (usually
        # this is the case, but it doesn't have to be)
        return os.path.join(self.post_process.rex._outputdir,
                            'cluster.%d' % self.cluster_num,
                            '%s.mrc' % component)

    def load_localization_density(self, state, component, asym):
        fname = self.get_localization_density_file(component)
        if os.path.exists(fname):
            details = "Localization density for %s %s" \
                      % (component, self.model_group.name)
            local_file = ihm.location.OutputFileLocation(fname,
                              details=state.get_postfixed_name(details))
            den = ihm.model.LocalizationDensity(file=local_file, asym_unit=asym)
            self.densities.append(den)

    def load_all_models(self, simo, state):
        stat_fname = self.post_process.get_stat_file(self.cluster_num)
        model_num = 0
        with open(stat_fname) as fh:
            stats = ast.literal_eval(fh.readline())
            # Correct path
            rmf_file = os.path.join(os.path.dirname(stat_fname),
                                    "%d.rmf3" % model_num)
            for c in state.all_modeled_components:
                # todo: this only works with PMI 1
                state._pmi_object.set_coordinates_from_rmf(c, rmf_file, 0,
                                                       force_rigid_update=True)
            # todo: fill in other data from stat file, e.g. crosslink phi/psi
            yield stats
            model_num += 1
            if model_num >= self.num_models_deposited:
                return

    # todo: also support dRMS precision
    def _get_precision(self):
        precfile = os.path.join(self.post_process.rex._outputdir,
                                "precision.%d.%d.out" % (self.cluster_num,
                                                         self.cluster_num))
        if not os.path.exists(precfile):
            return ihm.unknown
        # Fail if the precision.x.x.out file doesn't match the cluster
        r = re.compile('All .*/cluster.%d/ average centroid distance ([\d\.]+)'
                       % self.cluster_num)
        with open(precfile) as fh:
            for line in fh:
                m = r.match(line)
                if m:
                    return float(m.group(1))

    precision = property(lambda self: self._get_precision(),
                         lambda self, val: None)

class _SimpleEnsemble(ihm.model.Ensemble):
    """Simple manually-created ensemble"""

    num_models_deposited = None # Override base class property

    def __init__(self, pp, model_group, num_models, drmsd,
                 num_models_deposited, ensemble_file):
        super(_SimpleEnsemble, self).__init__(
                model_group=model_group, post_process=pp, num_models=num_models,
                file=ensemble_file, precision=drmsd, name=model_group.name,
                clustering_feature='dRMSD')
        self.num_models_deposited = num_models_deposited

    def load_localization_density(self, state, component, asym, local_file):
        den = ihm.model.LocalizationDensity(file=local_file, asym_unit=asym)
        self.densities.append(den)


class _EntityMapper(dict):
    """Handle mapping from IMP components to CIF entities.
       Multiple components may map to the same entity if they share sequence."""
    def __init__(self, system):
        super(_EntityMapper, self).__init__()
        self._sequence_dict = {}
        self._entities = []
        self.system = system

    def add(self, component_name, sequence):
        if sequence not in self._sequence_dict:
            # Use the name of the first component, stripped of any copy number,
            # as the description of the entity
            d = component_name.split("@")[0].split(".")[0]
            entity = ihm.Entity(sequence, description=d)
            self.system.entities.append(entity)
            self._sequence_dict[sequence] = entity
        self[component_name] = self._sequence_dict[sequence]


class _RestraintDataset(object):
    """Wrapper around a dataset associated with a restraint.
       This is needed because we need to delay access to the dataset
       in case the writer of the PMI script overrides or changes it
       after creating the restraint."""
    def __init__(self, restraint, num, allow_duplicates):
        self.restraint = restraint
        self.num, self.allow_duplicates = num, allow_duplicates
        self.__dataset = None
    def __get_dataset(self):
        if self.__dataset:
            return self.__dataset
        if self.num is not None:
            d = copy.deepcopy(self.restraint.datasets[self.num])
        else:
            d = copy.deepcopy(self.restraint.dataset)
        if self.allow_duplicates:
            d.location._allow_duplicates = True
        # Don't copy again next time we access self.dataset
        self.__dataset = d
        return d
    dataset = property(__get_dataset)

class _TransformedComponent(object):
    def __init__(self, name, original, transform):
        self.name, self.original, self.transform = name, original, transform


class _State(ihm.model.State):
    """Representation of a single state in the system."""

    def __init__(self, pmi_object, po):
        # Point to the PMI object for this state. Use a weak reference
        # since the state object typically points to us too, so we need
        # to break the reference cycle. In PMI1 this will be a
        # Representation object.
        self._pmi_object = weakref.proxy(pmi_object)
        self._pmi_state = pmi_object.state
        # Preserve PMI state name
        old_name = self.name
        super(_State, self).__init__(experiment_type='Fraction of bulk')
        self.name = old_name

        # The assembly of all components modeled by IMP in this state.
        # This may be smaller than the complete assembly.
        self.modeled_assembly = ihm.Assembly(
                        name="Modeled assembly",
                        description="All components modeled by IMP")
        po.system.orphan_assemblies.append(self.modeled_assembly)

        self.all_modeled_components = []

    def __hash__(self):
        return hash(self._pmi_state)
    def __eq__(self, other):
        return self._pmi_state == other._pmi_state

    def add_model_group(self, group):
        self.append(group)

    def get_prefixed_name(self, name):
        """Prefix the given name with the state name, if available."""
        if self.short_name:
            return self.short_name + ' ' + name
        else:
            # Can't use capitalize() since that un-capitalizes everything
            # but the first letter
            return name[0].upper() + name[1:] if name else ''

    def get_postfixed_name(self, name):
        """Postfix the given name with the state name, if available."""
        if self.short_name:
            return "%s in state %s" % (name, self.short_name)
        else:
            return name

    short_name = property(lambda self: self._pmi_state.short_name)
    long_name = property(lambda self: self._pmi_state.long_name)
    def __get_name(self):
        return self._pmi_state.long_name
    def __set_name(self, val):
        self._pmi_state.long_name = val
    name = property(__get_name, __set_name)


class _Representation(object):
    """A complete model representation"""
    def __init__(self, name):
        self.name = name


class ProtocolOutput(IMP.pmi.output.ProtocolOutput):
    """Class to encode a modeling protocol as mmCIF.

    IMP has basic support for writing out files in mmCIF format, for
    deposition in [PDB-dev](https://pdb-dev.rcsb.rutgers.edu/).
    After creating an instance of this class, attach it to an
    IMP.pmi.representation.Representation object. After this, any
    generated models and metadata are automatically collected and
    output as mmCIF.
    """
    def __init__(self, fh):
        # Ultimately, collect data in an ihm.System object
        self.system = ihm.System()
        self._state_group = ihm.model.StateGroup()
        self.system.state_groups.append(self._state_group)

        self.fh = fh
        self._state_ensemble_offset = 0
        self._each_metadata = [] # list of metadata for each representation
        self._file_datasets = []
        self._main_script = os.path.abspath(sys.argv[0])

        # Point to the main modeling script
        loc = ihm.location.WorkflowFileLocation(path=self._main_script,
                               details="The main integrative modeling script")
        self.system.locations.append(loc)

        self._states = {}
        self._working_directory = os.getcwd()
        self._cif_writer = ihm.format.CifWriter(fh)
        self._representations = []
        self.create_representation("Default representation")
        self.entities = _EntityMapper(self.system)
        # Mapping from component names to ihm.AsymUnit
        self.asym_units = {}
        self._all_components = {}
        self.all_modeled_components = []
        self._transformed_components = []
        self.sequence_dict = {}

        # Coordinates to exclude
        self._exclude_coords = {}

        self.model_repr_dump = _ModelRepresentationDumper(self)
        self.cross_link_dump = _CrossLinkDumper(self)
        self.model_prot_dump = _ModelProtocolDumper(self)
        self.all_datasets = _AllDatasets()
        self.starting_model_dump = _StartingModelDumper(self)

        self.model_dump = _ModelDumper(self)
        self.model_repr_dump.starting_model \
                    = self.starting_model_dump.starting_model
        self.all_software = _AllSoftware(self.system)
        self.post_process_dump = _PostProcessDumper(self)

        # Some dumpers add per-model information; give them a pointer to
        # the model list
        self.cross_link_dump.models = self.model_dump.models

        self._dumpers = [self.model_repr_dump,
                         self.cross_link_dump,
                         self.starting_model_dump,
                         # todo: detect atomic models and emit struct_conf
                         #_StructConfDumper(self),
                         self.model_prot_dump, self.post_process_dump,
                         self.model_dump]

    def create_representation(self, name):
        """Create a new Representation and return it. This can be
           passed to add_model(), add_bead_element() or add_pdb_element()."""
        r = _Representation(name)
        self._representations.append(r)
        r.id = len(self._representations)
        return r

    def exclude_coordinates(self, component, seqrange):
        """Don't record coordinates for the given domain.
           Coordinates for the given domain (specified by a component name
           and a 2-element tuple giving the start and end residue numbers)
           will be excluded from the mmCIF file. This can be used to exclude
           parts of the structure that weren't well resolved in modeling.
           Any bead or residue that lies wholly within this range will be
           excluded. Multiple ranges for a given component can be excluded
           by calling this method multiple times."""
        if component not in self._exclude_coords:
            self._exclude_coords[component] = []
        self._exclude_coords[component].append(seqrange)

    def _is_excluded(self, component, start, end):
        """Return True iff this chunk of sequence should be excluded"""
        for seqrange in self._exclude_coords.get(component, ()):
            if start >= seqrange[0] and end <= seqrange[1]:
                return True

    def _add_state(self, state):
        """Create a new state and return a pointer to it."""
        self._state_ensemble_offset = len(self.system.ensembles)
        s = _State(state, self)
        self._state_group.append(s)
        self._last_state = s
        return s

    def get_file_dataset(self, fname):
        for d in self._file_datasets:
            fd = d.get(os.path.abspath(fname), None)
            if fd:
                return fd

    def _get_chain_for_component(self, name, output):
        """Get the chain ID for a component, if any."""
        # todo: handle multiple copies
        if name in self.asym_units:
            return self.asym_units[name]._id
        else:
            # A non-modeled component doesn't have a chain ID
            return None

    def _get_assembly_comps(self, assembly):
        """Get the names of the components in the given assembly"""
        # component name = asym_unit.details
        comps = {}
        for ca in assembly:
            comps[ca.details] = None
        return comps

    def create_transformed_component(self, state, name, original, transform):
        assembly_comps = self._get_assembly_comps(state.modeled_assembly)
        if name in assembly_comps:
            raise ValueError("Component %s already exists" % name)
        elif original not in assembly_comps:
            raise ValueError("Original component %s does not exist" % original)
        self.create_component(state, name, True)
        self.add_component_sequence(state, name, self.sequence_dict[original])
        self._transformed_components.append(_TransformedComponent(
                                            name, original, transform))

    def create_component(self, state, name, modeled):
        new_comp = name not in self._all_components
        self._all_components[name] = None
        if modeled:
            state.all_modeled_components.append(name)
            if new_comp:
                self.asym_units[name] = None # assign asym once we get sequence
                self.all_modeled_components.append(name)

    def add_component_sequence(self, state, name, seq):
        if name in self.sequence_dict:
            if self.sequence_dict[name] != seq:
                raise ValueError("Sequence mismatch for component %s" % name)
        else:
            self.sequence_dict[name] = seq
            self.entities.add(name, seq)
        if name in self.asym_units and self.asym_units[name] is None:
            # Set up a new asymmetric unit for this component
            entity = self.entities[name]
            asym = ihm.AsymUnit(entity, details=name)
            self.system.asym_units.append(asym)
            state.modeled_assembly.append(asym)
            self.asym_units[name] = asym

    def _add_restraint_model_fits(self):
        """Add fits to restraints for all known models"""
        for m in self.model_dump.models:
            if m._is_restrained:
                for r in self.system.restraints:
                    if hasattr(r, 'add_fits_from_model_statfile'):
                        r.add_fits_from_model_statfile(m)

    def flush(self):
        self._add_restraint_model_fits()
        # Make sure ihm knows about final datasets and groups
        self.system.orphan_datasets.extend(
                           self.all_datasets._get_final_datasets())
        self.system.orphan_dataset_groups.extend(
                           self.all_datasets._get_final_groups())

        # Point all locations to repos, if applicable
        all_repos = [m for m in self._metadata
                     if isinstance(m, ihm.location.Repository)]
        self.system.update_locations_in_repositories(all_repos)

        # Dump out ihm-managed objects first
        ihm.dumper.write(self.fh, [self.system])
        # Now dump our own
        for dumper in self._dumpers:
            dumper.finalize_metadata()
        for dumper in self._dumpers:
            dumper.finalize()
        for dumper in self._dumpers:
            dumper.dump(self._cif_writer)

    def add_pdb_element(self, state, name, start, end, offset, pdbname,
                        chain, hier, representation=None):
        if self._is_excluded(name, start, end):
            return
        if representation is None:
            representation = self._representations[0]
        p = _PDBFragment(state, name, start, end, offset, pdbname, chain,
                         hier)
        self.model_repr_dump.add_fragment(state, representation, p)
        self.starting_model_dump.add_pdb_fragment(p)

    def add_bead_element(self, state, name, start, end, num, hier,
                         representation=None):
        if self._is_excluded(name, start, end):
            return
        if representation is None:
            representation = self._representations[0]
        b = _BeadsFragment(state, name, start, end, num, hier)
        self.model_repr_dump.add_fragment(state, representation, b)

    def _get_restraint_dataset(self, r, num=None, allow_duplicates=False):
        """Get a wrapper object for the dataset used by this restraint.
           This is needed because the restraint's dataset may be changed
           in the PMI script before we get to use it."""
        rs = _RestraintDataset(r, num, allow_duplicates)
        self._add_dataset(rs)
        return rs

    def get_cross_link_group(self, r):
        return _CrossLinkGroup(r, self._get_restraint_dataset(r))

    def add_experimental_cross_link(self, r1, c1, r2, c2, length, group):
        if c1 not in self._all_components or c2 not in self._all_components:
            # Crosslink refers to a component we didn't model
            # As a quick hack, just ignore it.
            # todo: need to add an entity for this anyway (so will need the
            # sequence, and add to struct_assembly)
            return None
        xl = _ExperimentalCrossLink(r1, c1, r2, c2, length, group)
        self.cross_link_dump.add_experimental(xl)
        return xl

    def add_cross_link(self, state, ex_xl, p1, p2, sigma1, sigma2, psi):
        self.cross_link_dump.add(_CrossLink(state, ex_xl, p1, p2, sigma1,
                                            sigma2, psi))

    def add_replica_exchange(self, state, rex):
        # todo: allow for metadata to say how many replicas were used in the
        # actual experiment, and how many independent runs were carried out
        # (use these as multipliers to get the correct total number of
        # output models)
        self.model_prot_dump.add_step(_ReplicaExchangeProtocolStep(state, rex))

    def _add_simple_dynamics(self, num_models_end, method):
        # Always assumed that we're dealing with the last state
        state = self._last_state
        self.model_prot_dump.add_step(_SimpleProtocolStep(state, num_models_end,
                                                          method))

    def _add_protocol(self):
        # Always assumed that we're dealing with the last state
        state = self._last_state
        self.model_prot_dump.add_protocol(state)

    def _add_dataset(self, dataset):
        return self.all_datasets.add(self._last_state, dataset)

    def _add_restraint_dataset(self, restraint):
        return self.all_datasets.add_restraint(self._last_state, restraint)

    def _add_simple_postprocessing(self, num_models_begin, num_models_end):
        # Always assumed that we're dealing with the last state
        state = self._last_state
        protocol = self.model_prot_dump.get_last_protocol(state)
        pp = _SimplePostProcess(protocol, num_models_begin, num_models_end)
        self.post_process_dump.add(pp)
        return pp

    def _add_no_postprocessing(self, num_models):
        # Always assumed that we're dealing with the last state
        state = self._last_state
        protocol = self.model_prot_dump.get_last_protocol(state)
        pp = _NoPostProcess(protocol, num_models)
        self.post_process_dump.add(pp)
        return pp

    def _add_simple_ensemble(self, pp, name, num_models, drmsd,
                             num_models_deposited, localization_densities,
                             ensemble_file):
        """Add an ensemble generated by ad hoc methods (not using PMI).
           This is currently only used by the Nup84 system."""
        # Always assumed that we're dealing with the last state
        state = self._last_state
        group = ihm.model.ModelGroup(name=state.get_prefixed_name(name))
        state.add_model_group(group)
        if ensemble_file:
            self.system.locations.append(ensemble_file)
        e = _SimpleEnsemble(pp, group, num_models, drmsd, num_models_deposited,
                            ensemble_file)
        self.system.ensembles.append(e)
        for c in state.all_modeled_components:
            den = localization_densities.get(c, None)
            if den:
                e.load_localization_density(state, c, self.asym_units[c], den)
        return e

    def set_ensemble_file(self, i, location):
        """Point a previously-created ensemble to an 'all-models' file.
           This could be a trajectory such as DCD, an RMF, or a multimodel
           PDB file."""
        self.system.locations.append(location)
        # Ensure that we point to an ensemble related to the current state
        ind = i + self._state_ensemble_offset
        self.system.ensembles[ind].file = location

    def add_replica_exchange_analysis(self, state, rex):
        # todo: add prefilter as an additional postprocess step (complication:
        # we don't know how many models it filtered)
        # todo: postpone rmsf/density localization extraction until after
        # relevant methods are called (currently it is done from the
        # constructor)
        protocol = self.model_prot_dump.get_last_protocol(state)
        num_models = protocol[-1].num_models_end
        pp = _ReplicaExchangeAnalysisPostProcess(protocol, rex, num_models)
        self.post_process_dump.add(pp)
        for i in range(rex._number_of_clusters):
            group = ihm.model.ModelGroup(name=state.get_prefixed_name(
                                                      'cluster %d' % (i + 1)))
            state.add_model_group(group)
            # todo: make # of models to deposit configurable somewhere
            e = _ReplicaExchangeAnalysisEnsemble(pp, i, group, 1)
            self.system.ensembles.append(e)
            # Add localization density info if available
            for c in state.all_modeled_components:
                e.load_localization_density(state, c, self.asym_units[c])
            for stats in e.load_all_models(self, state):
                m = self.add_model(group)
                # Since we currently only deposit 1 model, it is the
                # best scoring one
                m.name = 'Best scoring model'
                m.stats = stats
                # Add RMSF info if available
                for c in state.all_modeled_components:
                    e.load_rmsf(m, c)

    def _get_subassembly(self, comps, name, description):
        """Get an Assembly consisting of the given components.
           `compdict` is a dictionary of the components to add, where keys
           are the component names and values are the sequence ranges (or
           None to use all residues in the component)."""
        asyms = []
        for comp, seqrng in comps.items():
            a = self.asym_units[comp]
            asyms.append(a if seqrng is None else a(*seqrng))

        a = ihm.Assembly(asyms, name=name, description=description)
        return a

    def _add_foxs_restraint(self, model, comp, seqrange, dataset, rg, chi,
                            details):
        """Add a basic FoXS fit. This is largely intended for use from the
           NPC application."""
        assembly = self._get_subassembly({comp:seqrange},
                              name="SAXS subassembly",
                              description="All components that fit SAXS data")
        dataset = self._add_dataset(dataset)
        r = ihm.restraint.SASRestraint(dataset, assembly, segment=False,
                fitting_method='FoXS', fitting_atom_type='Heavy atoms',
                multi_state=False, radius_of_gyration=rg, details=details)
        r.fits[model] = ihm.restraint.SASRestraintFit(chi_value=chi)
        self.system.restraints.append(r)
        self._add_restraint_dataset(r) # so that all-dataset group works

    def add_em2d_restraint(self, state, r, i, resolution, pixel_size,
                           image_resolution, projection_number,
                           micrographs_number):
        r = _EM2DRestraint(state, r, i, resolution, pixel_size,
                           image_resolution, projection_number,
                           micrographs_number)
        self.system.restraints.append(r)
        self._add_restraint_dataset(r) # so that all-dataset group works

    def add_em3d_restraint(self, state, target_ps, densities, pmi_restraint):
        # todo: need to set allow_duplicates on this dataset?
        r = _EM3DRestraint(self, state, pmi_restraint, target_ps, densities)
        self.system.restraints.append(r)
        self._add_restraint_dataset(r) # so that all-dataset group works

    def add_model(self, group, assembly=None, representation=None):
        state = self._last_state
        if representation is None:
            representation = self._representations[0]
        return self.model_dump.add(state.prot,
                           self.model_prot_dump.get_last_protocol(state),
                           assembly if assembly else state.modeled_assembly,
                           representation, group)

    def _update_locations(self, filelocs):
        """Update FileLocation to point to a parent repository, if any"""
        all_repos = [m for m in self._metadata
                     if isinstance(m, ihm.location.Repository)]
        for fileloc in filelocs:
            ihm.location.Repository._update_in_repos(fileloc, all_repos)

    _metadata = property(lambda self:
                         itertools.chain.from_iterable(self._each_metadata))
